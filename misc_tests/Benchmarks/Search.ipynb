{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Database Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/vahid/Downloads/PhotonicsAI_Project\")\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from PhotonicsAI.Photon import llm_api, semantic_similarity, utils\n",
    "\n",
    "with open(\"../../Frontend/prompts.yaml\") as file:\n",
    "    prompts = yaml.safe_load(file)\n",
    "\n",
    "db_docs = utils.search_directory_for_docstrings(directory=\"../../KnowledgeBase\")\n",
    "list_of_docs = [i[\"docstring\"] for i in db_docs]\n",
    "list_of_cnames = [i[\"class_name\"] for i in db_docs]\n",
    "\n",
    "d = {\n",
    "    \"list_of_docs\": list_of_docs,\n",
    "    \"list_of_cnames\": list_of_cnames,\n",
    "}\n",
    "\n",
    "llm_api_selection_ = [\n",
    "    \"fireworks: llama-v3-70b-instruct\",\n",
    "    # 'fireworks: llama-v3-8b-instruct',\n",
    "    \"openai: gpt-4o\",\n",
    "    \"openai: gpt-3.5\",\n",
    "    #   'groq: llama3-70b',\n",
    "    # 'groq: llama3-8b',\n",
    "    \"fireworks: qwen2-72b-instruct\",\n",
    "    #   'fireworks: mixtral-8x22b-instruct',\n",
    "    \"google: gemini-1.5-flash\",\n",
    "    #   'google: gemini-1.5-pro',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Parse Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'A high speed modulator connected to a VOA'\n",
    "prompt = \"Five cascaded MZIs, each with two input and two output ports.\"\n",
    "\n",
    "\n",
    "def is_valid_yaml_custom(yaml_string: str) -> bool:\n",
    "    try:\n",
    "        data = yaml.safe_load(yaml_string)\n",
    "        [_] = list(data.values())\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "run_time = []\n",
    "valid_yaml = []\n",
    "N = 2\n",
    "for llm_api_selection in llm_api_selection_:\n",
    "    print(llm_api_selection)\n",
    "    c = 0\n",
    "    start_time = time.time()\n",
    "    for _i in range(N):\n",
    "        raw_component_list = llm_api.call_llm(\n",
    "            prompt, prompts[\"classify\"], llm_api_selection\n",
    "        )\n",
    "        c += is_valid_yaml_custom(raw_component_list)\n",
    "    end_time = time.time()\n",
    "    valid_yaml.append(c / N)\n",
    "    run_time.append((end_time - start_time) / N)\n",
    "\n",
    "print(\"----\")\n",
    "print(\"N=\", N)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": llm_api_selection_,\n",
    "        \"Valid YAML\": valid_yaml,\n",
    "        \"Time per request (s)\": run_time,\n",
    "    }\n",
    ")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Search Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[\"prompt\"] = \"Five cascaded MZIs, each with two input and two output ports.\"\n",
    "d[\"raw_component_list\"] = \"components:\\n  C1: MZI with two input and two output ports\"\n",
    "\n",
    "N = 50\n",
    "\n",
    "# Initialize a dictionary to hold the response data\n",
    "results = {}\n",
    "\n",
    "# Data collection and processing\n",
    "for llm_api_selection in llm_api_selection_:\n",
    "    d[\"llm_api_selection\"] = llm_api_selection\n",
    "    print(llm_api_selection)\n",
    "    response_counts = {}\n",
    "\n",
    "    for _ in range(N):\n",
    "        try:\n",
    "            db_component_list, modified_yaml_data_all = llm_api.yaml_components_search(\n",
    "                d\n",
    "            )\n",
    "            response = yaml.safe_load(db_component_list)[\"C1\"][\"component\"]\n",
    "        except:\n",
    "            response = \"ERROR!\"\n",
    "\n",
    "        if response in response_counts:\n",
    "            response_counts[response] += 1\n",
    "        else:\n",
    "            response_counts[response] = 1\n",
    "\n",
    "    results[llm_api_selection] = response_counts\n",
    "\n",
    "# Output results\n",
    "print(f\"\\nResults from {N} calls:\")\n",
    "for opt, counts in results.items():\n",
    "    print(f\"Model: {opt}\")\n",
    "    most_common_response = max(counts, key=counts.get)\n",
    "    print(f\"{counts[most_common_response]} times: {most_common_response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM retrieval: order matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "indices = list(range(len(list_of_docs)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "d = {\n",
    "    \"list_of_docs\": [list_of_docs[i] for i in indices],\n",
    "    \"list_of_cnames\": [list_of_cnames[i] for i in indices],\n",
    "}\n",
    "\n",
    "\n",
    "d[\"llm_api_selection\"] = llm_api_selection_[0]\n",
    "# d['prompt'] = 'Five cascaded MZIs, each with two input and two output ports.'\n",
    "d[\"raw_component_list\"] = \"components:\\n  C1: MZI with two input and two output ports\"\n",
    "# d['raw_component_list'] = llm_api.call_llm(d['prompt'], prompts['classify'], d['llm_api_selection'])\n",
    "\n",
    "db_component_list, modified_yaml_data_all = llm_api.yaml_components_search(d)\n",
    "print(d[\"raw_component_list\"])\n",
    "print()\n",
    "print(d[\"llm_api_selection\"])\n",
    "print()\n",
    "print(modified_yaml_data_all)\n",
    "print()\n",
    "print(d[\"list_of_cnames\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consistency check with 50 random documents ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "res = {}\n",
    "for llm_api_selection in llm_api_selection_:\n",
    "    res[llm_api_selection] = []\n",
    "    print(llm_api_selection)\n",
    "    for seed_ in range(50):\n",
    "        random.seed(seed_)\n",
    "\n",
    "        indices = list(range(len(list_of_docs)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        d = {\n",
    "            \"list_of_docs\": [list_of_docs[i] for i in indices],\n",
    "            \"list_of_cnames\": [list_of_cnames[i] for i in indices],\n",
    "        }\n",
    "        d[\"llm_api_selection\"] = llm_api_selection\n",
    "        d[\"raw_component_list\"] = (\n",
    "            \"components:\\n  C1: MZI with two input and two output ports\"\n",
    "        )\n",
    "\n",
    "        db_component_list, modified_yaml_data_all = llm_api.yaml_components_search(d)\n",
    "\n",
    "        data = yaml.safe_load(db_component_list)\n",
    "        ret_comp = list(data.values())[0][\"component\"]\n",
    "        res[llm_api_selection].append(ret_comp)\n",
    "\n",
    "print()\n",
    "for key, value in res.items():\n",
    "    freq = Counter(value)\n",
    "    print()\n",
    "    print(f\"{key}:\")\n",
    "    for item, count in sorted(freq.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"    {item}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d['prompt'] = 'Five cascaded MZIs, each with two input and two output ports.'\n",
    "d[\"prompt\"] = \"MZI with two input and two output ports\"\n",
    "# d['prompt'] = 'MZI'\n",
    "\n",
    "ii, ss = semantic_similarity.bm25(d[\"prompt\"], d[\"list_of_docs\"])\n",
    "# ii, ss = semantic_similarity.dragon(d['prompt'], d['list_of_docs'])\n",
    "# ii, ss = semantic_similarity.st(d['prompt'], d['list_of_docs'])\n",
    "\n",
    "sorted_docs = [d[\"list_of_cnames\"][i] for i in ii]\n",
    "sorted_scores = sorted(ss)[::-1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 2.2))\n",
    "plt.xlabel(d[\"prompt\"])\n",
    "bars = plt.bar(range(len(sorted_docs)), sorted_scores)\n",
    "plt.xticks([])\n",
    "\n",
    "for bar, label in zip(bars, sorted_docs):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        sorted_scores[0] * 0.1,\n",
    "        label,\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        rotation=90,\n",
    "        fontsize=8,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.gca().spines[\"top\"].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhotonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
