{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"C:/Users/vansari/Documents/PhotonicAI\")\n",
    "import pandas as pd\n",
    "import textgrad as tg\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "import yaml\n",
    "\n",
    "llm_engine = tg.get_engine(\"gpt-4o\")\n",
    "tg.set_backward_engine(llm_engine)\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"db/AMF_papers.parquet\")\n",
    "df[\"TextGrad_Nodes_preOptimize\"] = None\n",
    "df[\"TextGrad_Nodes_postOptimize\"] = None\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row[\"N_pages\"] < 20:\n",
    "        print(idx, \"=======================================\")\n",
    "\n",
    "        article = df.loc[idx][\"text_full\"]\n",
    "\n",
    "        problem_text = f\"\"\"Is this a single academic article, and not a dissertation or collection of papers (single_article)?\n",
    "        Is the main topic of this article about integrated photonic circuits (topic_photonic)?\n",
    "        If yes, find the photonic components that are used on the chip.\n",
    "        Return a concise list of these photonic components, if any (components_list).\n",
    "        For each component, try to extract: brief spec,\n",
    "        and the number of optical input (N) and output (M) ports denoted by NxM, e.g. 1x2.\n",
    "        Finally, is there an enough information to understand and desrcibe how the on-chip components\n",
    "        are interconnected to form the photonic circuit (circuit_complete)?\n",
    "        Answer in YAML following the template:\n",
    "        single_article: True/False\n",
    "        topic_photonic: True/False\n",
    "        components_list:\n",
    "        - a 1x1 modulator with MHz speed\n",
    "        - a 1x2 component ...\n",
    "        ...\n",
    "        circuit_complete: True/False\n",
    "\n",
    "\n",
    "        INPUT ARTICLE:\n",
    "        {article}\n",
    "        \"\"\"\n",
    "\n",
    "        problem = tg.Variable(\n",
    "            problem_text, role_description=\"the parsing problem\", requires_grad=False\n",
    "        )\n",
    "\n",
    "        model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "        code = model(problem)\n",
    "\n",
    "        df.at[idx, \"TextGrad_Nodes_preOptimize\"] = code.value\n",
    "\n",
    "        code.set_role_description(\"The yaml code to optimize\")\n",
    "        code.requires_grad = True\n",
    "\n",
    "        photonic_critic_prompt = \"\"\"You are a smart language model expert in photonic integrated circuits.\n",
    "        This YAML file should be an accurate summary of the photonic components presented in the input article.\n",
    "        Evaluate components_list in YAML based on:\n",
    "        - it should only represent photonic components on the chip, and not off the chip.\n",
    "        - does it accurately represent the photonic components?\n",
    "        - is any photonic component missing?\n",
    "        - this should be only a list of photonic components.\n",
    "        - does YAML follow the provided template?\n",
    "        Also evaluate circuit_complete boolean:\n",
    "        - is it correct? can you understand and describe the connection between items in components_list?\n",
    "        You do not propose a new YAML file, only evaluate the existing YAML file critically and give very concise feedback.\"\"\"\n",
    "        #\n",
    "        # TODO: check if output is a valid yaml\n",
    "        #\n",
    "        loss_system_prompt = tg.Variable(\n",
    "            photonic_critic_prompt,\n",
    "            requires_grad=False,\n",
    "            role_description=\"system prompt to the loss function\",\n",
    "        )\n",
    "\n",
    "        format_string = \"\"\"Problem: {problem}\\nCurrent YAML code: {code}\"\"\"\n",
    "        fields = {\"problem\": None, \"code\": None}\n",
    "        formatted_llm_call = tg.autograd.FormattedLLMCall(\n",
    "            engine=llm_engine,\n",
    "            format_string=format_string,\n",
    "            fields=fields,\n",
    "            system_prompt=loss_system_prompt,\n",
    "        )\n",
    "\n",
    "        def loss_fn(problem: tg.Variable, code: tg.Variable) -> tg.Variable:\n",
    "            inputs = {\"problem\": problem, \"code\": code}\n",
    "\n",
    "            return formatted_llm_call(\n",
    "                inputs=inputs,\n",
    "                response_role_description=f\"evaluation of the {code.get_role_description()}\",\n",
    "            )\n",
    "\n",
    "        loss = loss_fn(problem, code)\n",
    "        loss.backward()\n",
    "        optimizer = tg.TGD(parameters=[code])\n",
    "        optimizer.step()\n",
    "\n",
    "        print(code.value)\n",
    "        df.at[idx, \"TextGrad_Nodes_postOptimize\"] = code.value\n",
    "\n",
    "        # df.to_parquet('db/AMF_papers.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing output of TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"C:/Users/vansari/Documents/PhotonicAI\")\n",
    "sys.path.append(\"/Users/vahid/Downloads/PhotonicsAI_Project\")\n",
    "import pandas as pd\n",
    "import textgrad as tg\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "from PhotonicsAI.Photon import llm_api, utils\n",
    "\n",
    "df = pd.read_parquet(\"db/AMF_papers.parquet\")\n",
    "df.info()\n",
    "\n",
    "i = 0\n",
    "\n",
    "paper_nodes = {}\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row[\"TextGrad_Nodes_postOptimize\"]):\n",
    "        # print(idx, '=======================================')\n",
    "        nodes_ = row[\"TextGrad_Nodes_postOptimize\"]\n",
    "        nodes_ = nodes_.strip(\"```yaml\").strip(\"```\")\n",
    "        try:\n",
    "            nodes_ = yaml.safe_load(nodes_)\n",
    "            paper_nodes[idx] = nodes_\n",
    "        except:\n",
    "            i += 1\n",
    "        # osjdfojsdf\n",
    "\n",
    "print(\"---------------------\\n\")\n",
    "print(\"NOT yaml parsable: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../Photon/templates.yaml\") as file:\n",
    "    templates_dict = yaml.safe_load(file)\n",
    "    templates_str = yaml.dump(templates_dict, default_flow_style=False)\n",
    "\n",
    "# adding components to templates_dict:\n",
    "db_docs = utils.search_directory_for_docstrings(\"../../KnowledgeBase/DesignLibrary\")\n",
    "for i in db_docs:\n",
    "    templates_dict[i[\"module_name\"]] = i[\"docstring\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# searching PDK for components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_list = []\n",
    "templates_id_list = []\n",
    "for i, k in templates_dict.items():\n",
    "    templates_list.append(k)\n",
    "    templates_id_list.append(i)\n",
    "\n",
    "\n",
    "# dict_keys(['single_article', 'topic_photonic', 'components_list', 'circuit_complete'])\n",
    "\n",
    "c = 0\n",
    "for i, k in paper_nodes.items():\n",
    "    if k[\"single_article\"] & k[\"topic_photonic\"] & k[\"circuit_complete\"]:\n",
    "        c += 1\n",
    "        print(\"index: \", i)\n",
    "        paper_nodes[i][\"match_list\"] = []\n",
    "        paper_nodes[i][\"match_scores\"] = []\n",
    "        paper_nodes[i][\"match_comment\"] = []\n",
    "        for comp in k[\"components_list\"]:\n",
    "            comp = str(comp)\n",
    "            # result = query_pipeline.run({\"text_embedder\": {\"text\": comp}})\n",
    "            # print( f\"{comp} -----> {result['retriever']['documents'][0].id}\" )\n",
    "            r_llm = llm_api.llm_search(comp, templates_list)\n",
    "            print(f\"{comp} -----> {r_llm}\")\n",
    "\n",
    "            paper_nodes[i][\"match_list\"].append(r_llm.match_list)\n",
    "            paper_nodes[i][\"match_scores\"].append(r_llm.match_scores)\n",
    "            paper_nodes[i][\"match_comment\"].append(r_llm.match_comment)\n",
    "            # print(type(comp), re.sub(r\"[\\'{}]\", \"\", comp))\n",
    "        print(\"=======================\")\n",
    "\n",
    "        # sdfsd\n",
    "    # print(k.keys())\n",
    "\n",
    "print(\"papers with searched components: \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(templates_id_list[13])\n",
    "# print(templates_id_list[6])\n",
    "# print(templates_id_list[7])\n",
    "# print(templates_id_list[28])\n",
    "print(df.loc[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i, k in paper_nodes.items():\n",
    "    if k[\"single_article\"] & k[\"topic_photonic\"] & k[\"circuit_complete\"]:\n",
    "        for comp in k[\"components_list\"]:\n",
    "            c += 1\n",
    "            print(comp)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_complete_comp = []\n",
    "\n",
    "c = 0\n",
    "for idx, k in paper_nodes.items():\n",
    "    if k[\"single_article\"] & k[\"topic_photonic\"] & k[\"circuit_complete\"]:\n",
    "        all_components_present = True\n",
    "        for ll in k[\"components_matched\"]:\n",
    "            if len(ll) == 0:\n",
    "                all_components_present = False\n",
    "        if all_components_present:\n",
    "            idx_complete_comp.append(idx)\n",
    "        c += all_components_present\n",
    "        # print(comp)\n",
    "print(c)\n",
    "\n",
    "for i in idx_complete_comp:\n",
    "    try:\n",
    "        for ii in range(len(paper_nodes[i][\"components_list\"])):\n",
    "            print(paper_nodes[i][\"components_list\"][ii])\n",
    "            print(paper_nodes[i][\"components_matched\"][ii])\n",
    "        print(\"===========================\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"db/AMF_nodes_postTextGrad.pkl\", \"wb\") as f:\n",
    "    pickle.dump(paper_nodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"db/AMF_nodes_postTextGrad.pkl\", \"rb\") as f:\n",
    "    paper_nodes = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhotonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
