{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing pre- vs post- textgrad optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we promped gpt-4o with papers shorter that 20 pages, and ask to return this yaml:\n",
    "```yaml\n",
    "single_article: True/False\n",
    "topic_photonic: True/False\n",
    "components_list:\n",
    "- a 1x1 modulator with MHz speed\n",
    "- a 1x2 component ...\n",
    "...\n",
    "circuit_complete: True/False\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "table of summary:\n",
    "```\n",
    "all papers          374\n",
    "processed papers    321\n",
    "\n",
    "                                            pre         post\n",
    "valid yaml                                  312         308\n",
    "\n",
    "single_article & topic_photonic             303         302\n",
    "circuit_complete                            262         217\n",
    "\n",
    "total photonic components                   1206        1052\n",
    "valid photonic components                   79%         82%\n",
    "\n",
    "```\n",
    "\n",
    "---> Human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"C:/Users/vansari/Documents/PhotonicAI\")\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "import yaml\n",
    "\n",
    "from PhotonicsAI.Photon import llm_api\n",
    "\n",
    "df = pd.read_parquet(\"db/AMF_papers.parquet\")\n",
    "\n",
    "n = 0\n",
    "m = 0\n",
    "i = 0\n",
    "pre_nodes = {}\n",
    "for idx, row in df.iterrows():\n",
    "    m += 1\n",
    "    if pd.notna(row[\"TextGrad_Nodes_preOptimize\"]):\n",
    "        n += 1\n",
    "        try:\n",
    "            nodes_ = row[\"TextGrad_Nodes_preOptimize\"].strip(\"```yaml\").strip(\"```\")\n",
    "            pre_nodes[idx] = yaml.safe_load(nodes_)\n",
    "        except:\n",
    "            i += 1\n",
    "\n",
    "j = 0\n",
    "post_nodes = {}\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row[\"TextGrad_Nodes_postOptimize\"]):\n",
    "        try:\n",
    "            nodes_ = row[\"TextGrad_Nodes_postOptimize\"].strip(\"```yaml\").strip(\"```\")\n",
    "            post_nodes[idx] = yaml.safe_load(nodes_)\n",
    "        except:\n",
    "            j += 1\n",
    "\n",
    "print(\"all amf papers\", m)\n",
    "print(\"amf papers processed by textgrad (shorter than 20 pages)\", n)\n",
    "\n",
    "print(\"PRE: failed to parse component lists\", i)\n",
    "print(\"POST: failed to parse component lists\", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"counting the booleans:\")\n",
    "\n",
    "\n",
    "c = 0\n",
    "for i, k in pre_nodes.items():\n",
    "    # if k['single_article'] & k['topic_photonic']:\n",
    "    if k[\"circuit_complete\"]:\n",
    "        # if k['single_article'] & k['topic_photonic'] & k['circuit_complete']:\n",
    "        c += 1\n",
    "print(\"pre\", c)\n",
    "\n",
    "\n",
    "c = 0\n",
    "for i, k in post_nodes.items():\n",
    "    # if k['single_article'] & k['topic_photonic']:\n",
    "    if k[\"circuit_complete\"]:\n",
    "        # if k['single_article'] & k['topic_photonic'] & k['circuit_complete']:\n",
    "        c += 1\n",
    "print(\"post\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from rich.progress import track\n",
    "\n",
    "print(\"counting valid photonic compoenents\")\n",
    "\n",
    "\n",
    "class ComponentsResponse(BaseModel):\n",
    "    valid_photonic_components: bool\n",
    "\n",
    "\n",
    "sys_prompt1 = \"\"\"Is this yaml list of one or many strings.\n",
    "Each item in the list should describe an integrated photonic component.\n",
    "It should NOT be a electronic device or component.\n",
    "If correct, answer with valid_photonic_components.\n",
    "\"\"\"\n",
    "# It should NOT be a photonic device or component off the chip.\n",
    "\n",
    "c = 0\n",
    "cc = 0\n",
    "for k, v in track(pre_nodes.items()):\n",
    "    if (\n",
    "        pre_nodes[k][\"single_article\"]\n",
    "        & pre_nodes[k][\"topic_photonic\"]\n",
    "        & pre_nodes[k][\"circuit_complete\"]\n",
    "    ):\n",
    "        j = yaml.dump(v[\"components_list\"])\n",
    "        r = llm_api.callgpt_pydantic(j, sys_prompt1, ComponentsResponse)\n",
    "        pre_nodes[k][\"valid_photonic_components\"] = r.valid_photonic_components\n",
    "        if r.valid_photonic_components:\n",
    "            c += len(v[\"components_list\"])\n",
    "        else:\n",
    "            print(j)\n",
    "            print(\"---------------------\")\n",
    "        cc += len(v[\"components_list\"])\n",
    "\n",
    "print(\"PRE: total photonic components:\", cc)\n",
    "print(\"PRE: valid photonic components:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 0\n",
    "c = 0\n",
    "\n",
    "for k, v in track(post_nodes.items()):\n",
    "    if (\n",
    "        post_nodes[k][\"single_article\"]\n",
    "        & post_nodes[k][\"topic_photonic\"]\n",
    "        & post_nodes[k][\"circuit_complete\"]\n",
    "    ):\n",
    "        j = yaml.dump(v[\"components_list\"])\n",
    "        r = llm_api.callgpt_pydantic(j, sys_prompt1, ComponentsResponse)\n",
    "        post_nodes[k][\"valid_photonic_components\"] = r.valid_photonic_components\n",
    "        if r.valid_photonic_components:\n",
    "            c += len(v[\"components_list\"])\n",
    "        else:\n",
    "            print(j)\n",
    "            print(\"---------------------\")\n",
    "        cc += len(v[\"components_list\"])\n",
    "\n",
    "print(\"POST: total photonic components:\", cc)\n",
    "print(\"POST: valid photonic components:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "all_pre_nodes = []\n",
    "for k, v in pre_nodes.items():\n",
    "    if (\n",
    "        pre_nodes[k][\"single_article\"]\n",
    "        & pre_nodes[k][\"topic_photonic\"]\n",
    "        & pre_nodes[k][\"circuit_complete\"]\n",
    "    ):\n",
    "        for i in v[\"components_list\"]:\n",
    "            all_pre_nodes.append(str(i))\n",
    "\n",
    "all_post_nodes = []\n",
    "for k, v in post_nodes.items():\n",
    "    if (\n",
    "        post_nodes[k][\"single_article\"]\n",
    "        & post_nodes[k][\"topic_photonic\"]\n",
    "        & post_nodes[k][\"circuit_complete\"]\n",
    "    ):\n",
    "        for i in v[\"components_list\"]:\n",
    "            all_post_nodes.append(str(i))\n",
    "\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "all_pre_nodes_embeddings = embeddings_model.embed_documents(all_pre_nodes)\n",
    "all_post_nodes_embeddings = embeddings_model.embed_documents(all_post_nodes)\n",
    "\n",
    "print(len(all_post_nodes_embeddings))\n",
    "print(all_post_nodes_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5)\n",
    "clustering_model.fit(all_post_nodes_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "post_clustered = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in post_clustered:\n",
    "        post_clustered[cluster_id] = []\n",
    "\n",
    "    post_clustered[cluster_id].append(all_post_nodes[sentence_id])\n",
    "\n",
    "for i, cluster in post_clustered.items():\n",
    "    print(\"==========================\", i)\n",
    "    for k in cluster:\n",
    "        print(k)\n",
    "    print(\"==========================\")\n",
    "\n",
    "print(\"===============\")\n",
    "print(len(post_clustered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5)\n",
    "clustering_model.fit(all_pre_nodes_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "pre_clustered = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in pre_clustered:\n",
    "        pre_clustered[cluster_id] = []\n",
    "\n",
    "    pre_clustered[cluster_id].append(all_pre_nodes[sentence_id])\n",
    "\n",
    "for i, cluster in pre_clustered.items():\n",
    "    print(\"==========================\", i)\n",
    "    for k in cluster:\n",
    "        print(k)\n",
    "    print(\"==========================\")\n",
    "\n",
    "print(\"===============\")\n",
    "print(len(pre_clustered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhotonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
